{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dff634f-8241-4a38-ad49-a3daa42aa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "import numpy as np\n",
    "\n",
    "import os                                                      #Needed to move between OS folders\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup                                  #Scraper \n",
    "import requests                                                #URL drainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime                                  #To be leveraged to define datetime objects  \n",
    "\n",
    "\n",
    "import nltk                                                    #Text processing library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')                                         #Download command on NLTK library to get specific \n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    #Useful already implemented tfidf vectorizer from scikit learn library\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep                                  #MapReduce methods to perform the map-shuffle-reduce pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe6532-7b2b-43fc-bbf1-b282d87c4cb4",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c53cba-0b9d-4db4-a286-aaeb7c0bb170",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa368-8991-4b48-8ef8-21f99eff73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "\n",
    "for i in range(1,401):\n",
    "    main_url = 'https://www.atlasobscura.com/places?page=' + str(i) +'&sort=likes_count'\n",
    "    cont = requests.get(main_url)\n",
    "    soup = BeautifulSoup(cont.text)\n",
    "    for place in soup.find_all('a', {'class':'content-card content-card-place'}):\n",
    "        places.append('https://www.atlasobscura.com'+place.get('href'))\n",
    "\n",
    "f = open('places.txt','w+')\n",
    "\n",
    "for place in places:\n",
    "    f.write(place+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d4b30-cacf-4157-bba6-f135886f9b71",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb929137-405c-4738-a256-d8e887c6d351",
   "metadata": {},
   "source": [
    "## 1.2 Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034aa15-b321-47e6-9200-7c5f87340f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('places.txt','r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "dic = {}\n",
    "for i in range(0,7200,18):\n",
    "    dic[1+i//18] = lines[i:i+18]\n",
    "\n",
    "for page in range(0,401):\n",
    "    try:\n",
    "        os.mkdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "\n",
    "    for place in dic[page]:\n",
    "        place_name = place[36:len(place)-1]\n",
    "        vanilla = requests.get(place[:-1],allow_redirects=False,headers = {'User-agent': 'your bot 0.1'})\n",
    "        \n",
    "        with open(place_name+\".txt\",'w+',encoding=\"utf-8\") as new_file:\n",
    "            new_file.write(vanilla.text)\n",
    "\n",
    "os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430dcbd-e617-4b4b-9c90-0796e0cbad59",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0459e40-1e7d-4a14-987b-855837102c60",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a141e-07c6-4ef8-b2d8-7508fb9f5a81",
   "metadata": {},
   "source": [
    "We need to define a script to extract useful information from each HTML we collected. In particular we want these information:\n",
    "1. Place Name (to save as $placeName$): String.\n",
    "2. Place Tags (to save as $placeTags$): List of Strings.\n",
    "3. Number of people who have been there (to save as $numPeopleVisited$): Integer.\n",
    "4. Number of people who want to visit the place(to save as $numPeopleWant$): Integer.\n",
    "5. Description (to save as $placeDesc$): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as $placeShortDesc$): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as $placeNearby$): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as $placeAddress$): String.\n",
    "9. Altitude and Longitude of the place's location(to save as $placeAlt$ and $placeLong$): Integers\n",
    "10. The username of the post editors (to save as $placeEditors$): List of Strings.\n",
    "11. Post publishing date (to save as $placePubDate$): datetime.\n",
    "12. The names of the lists that the place was included in (to save as $placeRelatedLists$): List of Strings.\n",
    "13. The names of the related places (to save as $placeRelatedPlaces$): List of Strings.\n",
    "14. The URL of the page of the place (to save as $placeURL$):String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2494-ef9a-4781-937f-225005b6928e",
   "metadata": {},
   "source": [
    "We leverage *BeautifulSoup library* to scrape the information, but we need just an additional method to convert into a datetime object the post publishing date since it was a string in the format 'Month Day, Year'. For example we found 'May 8, 2010' for the very first link, and instead we wanted '2010/05/08'. This method does so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554ae59c-ecbf-4ad3-bc06-5f5f97e8550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(string):\n",
    "    return str(datetime.strptime(string, '%B %d, %Y').date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba77953-7815-46b1-a045-eb77941ce15d",
   "metadata": {},
   "source": [
    "We define a function that builds a dictionary of information for every place we go through: then from this dictionary we'll buil a .tsv for every HTML document we gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17c331df-bd9f-4ac9-ba0a-ba7f16196880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darkAtlasScraper(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text)\n",
    "    \n",
    "    scraped = {'placeName': 'NaN',\n",
    "               'placeTags': 'NaN',\n",
    "               'numPeopleVisited': 'NaN',\n",
    "               'numPeopleWant': 'NaN',\n",
    "               'placeDesc': 'NaN',\n",
    "               'placeShortDesc':'Nan',\n",
    "               'placeNearby': 'NaN',\n",
    "               'placeAddress': 'NaN',\n",
    "               'placeAlt': 'NaN',\n",
    "               'placeLong': 'NaN',\n",
    "               'placeEditors': 'NaN',\n",
    "               'placePubDate': 'NaN',\n",
    "               'placeRelatedPlaces': 'NaN',\n",
    "               'placeRelatedLists': 'NaN',\n",
    "               'placeURL': 'NaN'}          \n",
    "    \n",
    "    try:\n",
    "        scraped['placeName'] = soup.find_all('h1',{'class':'DDPage__header-title'})[0].contents[0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "           \n",
    "    try:\n",
    "        scraped['placeTags'] = list(map(lambda s:s.strip(),\n",
    "                                        [tag.contents[0] for tag in soup.find_all('a',{'class':'itemTags__link js-item-tags-link'})]))\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    counters = soup.find_all('div',{'class':'title-md item-action-count'})\n",
    "    try:\n",
    "        scraped['numPeopleVisited'] = int(counters[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        scraped['numPeopleWant'] = int(counters[1].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    place_desc = ''\n",
    "    for paragraph in soup.find_all('div',{'class':'DDP__body-copy'})[0].find_all('p'):\n",
    "        for element in paragraph.contents:\n",
    "            if re.search('<[^>]*>', str(element)):\n",
    "                element = re.sub('<[^>]*>', \"\", str(element))\n",
    "                place_desc += element\n",
    "            else:\n",
    "                place_desc += str(element)\n",
    "    scraped['placeDesc'] = place_desc\n",
    "    \n",
    "    try:\n",
    "        scraped['placeShortDesc'] = soup.find_all('h3',{'class':'DDPage__header-dek'})[0].contents[0].replace(u'\\xa0', u'')\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    nearby = []\n",
    "    try:\n",
    "        for nearbies in soup.find_all('div',{'class':'DDPageSiderailRecirc__item-text'}):\n",
    "            nearby.append(nearbies.find_all('div',{'class':'DDPageSiderailRecirc__item-title'})[0].contents[0])\n",
    "        scraped['placeNearby'] = nearby\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        address = (str(soup.find_all('aside',{'class':'DDPageSiderail__details'})[0]\n",
    "                           .find_all('address',{'class':'DDPageSiderail__address'})[0]\n",
    "                           .find_all('div')[0])\n",
    "                           .split('\\n', 1)[0])\n",
    "        scraped['placeAddress'] = re.sub('<[^>]*>', \" \", address)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    coordinates = soup.find_all('div',{'class':'DDPageSiderail__coordinates js-copy-coordinates'})[0].contents[2]\n",
    "    scraped['placeAlt'] = float(coordinates.split()[0][:-1])\n",
    "    scraped['placeLong'] = float(coordinates.split()[1])\n",
    "\n",
    "\n",
    "    editorsoup = soup.find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "    scraped['placeEditors'] = [stuff.find_all('span')[0].contents[0] \n",
    "                               for stuff in editorsoup \n",
    "                               if len(stuff.find_all('span')) > 0]\n",
    "    if not scraped['placeEditors']:\n",
    "        zzz = soup.find_all('div',{'class':'ugc-editors'})\n",
    "        flag = 0\n",
    "        for soupper in zzz:\n",
    "            if soupper.find_all('h6')[0].contents[0] == 'Added by':\n",
    "                flag = 1\n",
    "                break\n",
    "        try:\n",
    "            editorsoup = soup.find_all('div',{'class':'ugc-editors'})[flag].find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "            scraped['placeEditors'] = [editors.contents[0]\n",
    "                                       for editors in editorsoup]\n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "    try:\n",
    "        scraped['placePubDate'] = string_to_datetime(soup.find_all('div',{'class':'DDPContributor__name'})[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    kircher = soup.find_all('div',{'class':'athanasius'})\n",
    "    for piece in kircher:\n",
    "        for piecer in piece.find_all('div',{'class':'CardRecircSection__title'}):\n",
    "            if piecer.contents[0] == 'Related Places':\n",
    "                scraped['placeRelatedPlaces'] = [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "            elif 'Appears in' in piecer.contents[0]:\n",
    "                scraped['placeRelatedLists'] =  [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "    \n",
    "    scraped['placeURL'] = 'https://www.atlasobscura.com/places/' + filename[:-4]\n",
    "    \n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb53fea-41ad-4584-84ea-8580afffcb0b",
   "metadata": {},
   "source": [
    "Now we have to define the script that goes through each folder and for each folder goes through each downloaded HTML, scrapes information and store them in a new .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fa2407d-1215-48a3-83a5-313be2db3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,401):\n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "    \n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "        \n",
    "        new_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page) + '\\\\' + filename\n",
    "        soupper = open(new_path, 'r',encoding=\"utf-8\")\n",
    "        \n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv')\n",
    "        newer_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv\\\\'+filename[:-4]+'.tsv'\n",
    "        try:\n",
    "            infos = darkAtlasScraper(soupper)\n",
    "        except IndexError:\n",
    "            print(newer_path)\n",
    "        with open(newer_path,'w+',encoding=\"utf-8\") as new_file:\n",
    "            for info in infos.values():\n",
    "                if type(info) == list:\n",
    "                    for index in range(len(info)):\n",
    "                        if index < len(info) - 1:\n",
    "                            new_file.write(str(info[index])+ ', ')\n",
    "                        elif index == len(info) - 1:\n",
    "                            new_file.write(str(info[index]))\n",
    "                    new_file.write('\\t')\n",
    "                else:\n",
    "                    new_file.write(str(info))\n",
    "                    new_file.write('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a4d4b-29bd-40c4-bb87-ece21839d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Leonardo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded8d3",
   "metadata": {},
   "source": [
    "We then decided to build a .csv to collect all the data and to make them always available in a practical format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59a1fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= r\"C:\\Users\\Leonardo\\ADM_HW3\\tsv\"\n",
    "final_dataset = []\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "for file in filenames:\n",
    "    if file.endswith('tsv'):\n",
    "        file_path = os.path.join(path,file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep =\"\\t\", header=None, quoting=3)\n",
    "            final_dataset.append(df)\n",
    "        except:\n",
    "            print(file)\n",
    "            pass\n",
    "    \n",
    "final_dataset = pd.concat(final_dataset)\n",
    "final_dataset.to_csv('final_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdac034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Leonardo\\Documents\\GitHub\\3HW-ADM-Fabri.Dinino.Aur\\final_dataset.csv\"\n",
    "\n",
    "mostPopularPlaces = pd.read_csv(path)\n",
    "mostPopularPlaces = mostPopularPlaces.iloc[:, :-2]\n",
    "mostPopularPlaces.columns = ['placeName', 'placeTags', 'numPeopleVisited', 'numPeopleWant', 'placeDesc', 'placeShortDesc', 'placeNearby', \n",
    "        'placeAddress', 'placeAlt', 'placeLong', 'placeEditors', 'placePubDate', 'placeRelatedPlaces', 'placeRelatedLists', 'placeURL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50ebb1-6b01-4e0c-ac62-d6d364b9ba9c",
   "metadata": {},
   "source": [
    "We now have the whole dataset and we are ready to start building the search engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1673fa-cf2d-494b-b215-e3f4468bf727",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1023031",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea00f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')                           #Download command on NLTK library to get specific \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb97ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntlk_analysis(info):\n",
    "    final_words = []\n",
    "    tokens = word_tokenize(info.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token.encode().isalpha():\n",
    "            stemming_token = ps.stem(token)\n",
    "            final_words.append(stemming_token)\n",
    "            \n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87f2d1",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd48469",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords = []\n",
    "for index, row in mostPopularPlaces['placeDesc'].iteritems():\n",
    "    description = mostPopularPlaces['placeDesc'][index]\n",
    "    listOfWords.append(ntlk_analysis(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0027273",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopularPlaces['listOfWords'] = listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9821576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [need, drop, research, facil, exist, address, ...\n",
       "1     [walk, street, forgiven, think, parisian, buil...\n",
       "2     [outsid, bluff, utah, massiv, alcov, loom, san...\n",
       "3     [receiv, fanfar, today, small, town, aurora, t...\n",
       "4     [nestl, ground, floor, unesco, world, heritag,...\n",
       "5     [amidst, glut, shop, restaur, make, san, diego...\n",
       "6     [loui, flight, cage, built, world, fair, year,...\n",
       "7     [commerci, hub, excit, new, way, travel, art, ...\n",
       "8     [home, futur, past, featur, aqua, tile, bathro...\n",
       "9     [pope, john, paul, ii, gift, use, santi, vince...\n",
       "10    [beeton, christma, annual, huge, popular, vict...\n",
       "11    [updat, hour, church, elvi, longer, physic, lo...\n",
       "12    [could, quit, possibl, frivol, yet, excit, adv...\n",
       "13    [els, new, york, citi, would, find, stunningli...\n",
       "14    [airplan, aficionado, rejoic, heaven, found, g...\n",
       "Name: listOfWords, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostPopularPlaces['listOfWords'].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ce52b",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b65b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabolary = {}\n",
    "\n",
    "for index, list_ in mostPopularPlaces['listOfWords'].iteritems():\n",
    "    for word in list_:\n",
    "        if word in vocabolary:\n",
    "            if index not in vocabolary[word]:\n",
    "                vocabolary[word].append(index)\n",
    "        else:\n",
    "            vocabolary[word] = [index]\n",
    "\n",
    "vocabolary = dict(sorted(vocabolary.items()))\n",
    "with open('vocabolary.txt', 'w') as file:\n",
    "    file.write(str(vocabolary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ed274",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_match(query,df):\n",
    "    result = []\n",
    "    query = ntlk_analysis(query)\n",
    "    match = {key: [] for key in query}\n",
    "\n",
    "    for key,list_of_values in vocabolary.items():\n",
    "        if key in match:\n",
    "            for value in list_of_values:\n",
    "                if value not in match[key]:\n",
    "                    match[key].append(value)\n",
    "                    \n",
    "    final_values = list(match.values())\n",
    "    intersection = set.intersection(*map(set,final_values))\n",
    "    \n",
    "    for index in intersection:\n",
    "        series = df[['placeName', 'placeDesc','placeURL']].loc[index]\n",
    "        result.append(series)\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2eccf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>National Museum of Health and Medicine</td>\n",
       "      <td>Once housed in downtown Washington, D.C., the ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/national-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>National Museum of the Pacific War</td>\n",
       "      <td>Dedicated specifically to remembering the stor...</td>\n",
       "      <td>https://www.atlasobscura.com/places/national-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6147</th>\n",
       "      <td>The Old Patent Model Museum</td>\n",
       "      <td>Before the Smithsonian reopened the building i...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-old-pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>National Watch and Clock Museum</td>\n",
       "      <td>In our age, state-of-the-art clocks keep time ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/national-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>Fraunces Tavern</td>\n",
       "      <td>Fraunces Tavern was originally the home of ear...</td>\n",
       "      <td>https://www.atlasobscura.com/places/fraunces-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>National Cowboy and Western Heritage Museum</td>\n",
       "      <td>To truly understand the era and the land that ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/national-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>National Cryptologic Museum</td>\n",
       "      <td>Update as of October 2021: Currently closed fo...</td>\n",
       "      <td>https://www.atlasobscura.com/places/national-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>Catoctin Furnace</td>\n",
       "      <td>In the early 1770s, Thomas Johnson discovered ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/catoctin-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6653</th>\n",
       "      <td>Urology Museum</td>\n",
       "      <td>It wasn’t long a go that a bladder stone was a...</td>\n",
       "      <td>https://www.atlasobscura.com/places/urology-mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>Sweet Home Cafe</td>\n",
       "      <td>Thomas Downing was the oyster king. In 19th-ce...</td>\n",
       "      <td>https://www.atlasobscura.com/places/sweet-home...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        placeName  \\\n",
       "4097       National Museum of Health and Medicine   \n",
       "4098           National Museum of the Pacific War   \n",
       "6147                  The Old Patent Model Museum   \n",
       "4100             National Watch and Clock Museum    \n",
       "2053                              Fraunces Tavern   \n",
       "...                                           ...   \n",
       "4085  National Cowboy and Western Heritage Museum   \n",
       "4086                  National Cryptologic Museum   \n",
       "1018                             Catoctin Furnace   \n",
       "6653                               Urology Museum   \n",
       "5631                              Sweet Home Cafe   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "4097  Once housed in downtown Washington, D.C., the ...   \n",
       "4098  Dedicated specifically to remembering the stor...   \n",
       "6147  Before the Smithsonian reopened the building i...   \n",
       "4100  In our age, state-of-the-art clocks keep time ...   \n",
       "2053  Fraunces Tavern was originally the home of ear...   \n",
       "...                                                 ...   \n",
       "4085  To truly understand the era and the land that ...   \n",
       "4086  Update as of October 2021: Currently closed fo...   \n",
       "1018  In the early 1770s, Thomas Johnson discovered ...   \n",
       "6653  It wasn’t long a go that a bladder stone was a...   \n",
       "5631  Thomas Downing was the oyster king. In 19th-ce...   \n",
       "\n",
       "                                               placeURL  \n",
       "4097  https://www.atlasobscura.com/places/national-m...  \n",
       "4098  https://www.atlasobscura.com/places/national-m...  \n",
       "6147  https://www.atlasobscura.com/places/the-old-pa...  \n",
       "4100  https://www.atlasobscura.com/places/national-w...  \n",
       "2053  https://www.atlasobscura.com/places/fraunces-t...  \n",
       "...                                                 ...  \n",
       "4085  https://www.atlasobscura.com/places/national-c...  \n",
       "4086  https://www.atlasobscura.com/places/national-c...  \n",
       "1018  https://www.atlasobscura.com/places/catoctin-f...  \n",
       "6653  https://www.atlasobscura.com/places/urology-mu...  \n",
       "5631  https://www.atlasobscura.com/places/sweet-home...  \n",
       "\n",
       "[237 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_search_engine = search_match('american museum', mostPopularPlaces)\n",
    "first_search_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8113784",
   "metadata": {},
   "source": [
    "# 2.2. Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbf2c3",
   "metadata": {},
   "source": [
    "We can leverage tfidf method from sklearn, or we can implement the tfidf from scratch. We will implement both solution as exercise, but we'll use *TfdifVectorizer* to focus on tuning the model on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a914e5ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m, lowercase\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, tokenizer\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m text:text)\n\u001b[1;32m----> 2\u001b[0m result \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mfit_transform(webdata\u001b[39m.\u001b[39mlist_words)\n\u001b[0;32m      3\u001b[0m tfidf\u001b[39m.\u001b[39mget_feature_names()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdata' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input='content', lowercase=False, tokenizer=lambda text:text)\n",
    "result = tfidf.fit_transform(webdata.list_words)\n",
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20674b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cdd751ff1a78921eff5cc54f687db2b577836ff9e523662c7eedaeda872ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
