{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dff634f-8241-4a38-ad49-a3daa42aa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe6532-7b2b-43fc-bbf1-b282d87c4cb4",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c53cba-0b9d-4db4-a286-aaeb7c0bb170",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa368-8991-4b48-8ef8-21f99eff73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "\n",
    "for i in range(1,401):\n",
    "    main_url = 'https://www.atlasobscura.com/places?page=' + str(i) +'&sort=likes_count'\n",
    "    cont = requests.get(main_url)\n",
    "    soup = BeautifulSoup(cont.text)\n",
    "    for place in soup.find_all('a', {'class':'content-card content-card-place'}):\n",
    "        places.append('https://www.atlasobscura.com'+place.get('href'))\n",
    "\n",
    "f = open('places.txt','w+')\n",
    "\n",
    "for place in places:\n",
    "    f.write(place+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d4b30-cacf-4157-bba6-f135886f9b71",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb929137-405c-4738-a256-d8e887c6d351",
   "metadata": {},
   "source": [
    "## 1.2 Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034aa15-b321-47e6-9200-7c5f87340f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('places.txt','r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "dic = {}\n",
    "for i in range(0,7200,18):\n",
    "    dic[1+i//18] = lines[i:i+18]\n",
    "\n",
    "for page in range(0,401):\n",
    "    try:\n",
    "        os.mkdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "\n",
    "    for place in dic[page]:\n",
    "        place_name = place[36:len(place)-1]\n",
    "        vanilla = requests.get(place[:-1],allow_redirects=False,headers = {'User-agent': 'your bot 0.1'})\n",
    "        \n",
    "        with open(place_name+\".txt\",'w+',encoding=\"utf-8\") as new_file:\n",
    "            new_file.write(vanilla.text)\n",
    "\n",
    "os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430dcbd-e617-4b4b-9c90-0796e0cbad59",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0459e40-1e7d-4a14-987b-855837102c60",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a141e-07c6-4ef8-b2d8-7508fb9f5a81",
   "metadata": {},
   "source": [
    "We need to define a script to extract useful information from each HTML we collected. In particular we want these information:\n",
    "1. Place Name (to save as $placeName$): String.\n",
    "2. Place Tags (to save as $placeTags$): List of Strings.\n",
    "3. Number of people who have been there (to save as $numPeopleVisited$): Integer.\n",
    "4. Number of people who want to visit the place(to save as $numPeopleWant$): Integer.\n",
    "5. Description (to save as $placeDesc$): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as $placeShortDesc$): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as $placeNearby$): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as $placeAddress$): String.\n",
    "9. Altitude and Longitude of the place's location(to save as $placeAlt$ and $placeLong$): Integers\n",
    "10. The username of the post editors (to save as $placeEditors$): List of Strings.\n",
    "11. Post publishing date (to save as $placePubDate$): datetime.\n",
    "12. The names of the lists that the place was included in (to save as $placeRelatedLists$): List of Strings.\n",
    "13. The names of the related places (to save as $placeRelatedPlaces$): List of Strings.\n",
    "14. The URL of the page of the place (to save as $placeURL$):String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2494-ef9a-4781-937f-225005b6928e",
   "metadata": {},
   "source": [
    "We leverage *BeautifulSoup library* to scrape the information, but we need just an additional method to convert into a datetime object the post publishing date since it was a string in the format 'Month Day, Year'. For example we found 'May 8, 2010' for the very first link, and instead we wanted '2010/05/08'. This method does so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554ae59c-ecbf-4ad3-bc06-5f5f97e8550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(string):\n",
    "    return str(datetime.strptime(string, '%B %d, %Y').date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba77953-7815-46b1-a045-eb77941ce15d",
   "metadata": {},
   "source": [
    "We define a function that builds a dictionary of information for every place we go through: then from this dictionary we'll buil a .tsv for every HTML document we gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17c331df-bd9f-4ac9-ba0a-ba7f16196880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darkAtlasScraper(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text)\n",
    "    \n",
    "    scraped = {'placeName': 'NaN',\n",
    "               'placeTags': 'NaN',\n",
    "               'numPeopleVisited': 'NaN',\n",
    "               'numPeopleWant': 'NaN',\n",
    "               'placeDesc': 'NaN',\n",
    "               'placeNearby': 'NaN',\n",
    "               'placeAddress': 'NaN',\n",
    "               'placeAlt': 'NaN',\n",
    "               'placeLong': 'NaN',\n",
    "               'placeEditors': 'NaN',\n",
    "               'placePubDate': 'NaN',\n",
    "               'placeRelatedPlaces': 'NaN',\n",
    "               'placeRelatedLists': 'NaN',\n",
    "               'placeURL': 'NaN'}          \n",
    "    \n",
    "    try:\n",
    "        scraped['placeName'] = soup.find_all('h1',{'class':'DDPage__header-title'})[0].contents[0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "           \n",
    "    try:\n",
    "        scraped['placeTags'] = list(map(lambda s:s.strip(),\n",
    "                                        [tag.contents[0] for tag in soup.find_all('a',{'class':'itemTags__link js-item-tags-link'})]))\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    counters = soup.find_all('div',{'class':'title-md item-action-count'})\n",
    "    try:\n",
    "        scraped['numPeopleVisited'] = int(counters[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        scraped['numPeopleWant'] = int(counters[1].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    place_desc = ''\n",
    "    for paragraph in soup.find_all('div',{'class':'DDP__body-copy'})[0].find_all('p'):\n",
    "        for element in paragraph.contents:\n",
    "            if re.search('<[^>]*>', str(element)):\n",
    "                element = re.sub('<[^>]*>', \"\", str(element))\n",
    "                place_desc += element\n",
    "            else:\n",
    "                place_desc += str(element)\n",
    "    scraped['placeDesc'] = place_desc\n",
    "    \n",
    "    try:\n",
    "        scraped['placeShortDesc'] = soup.find_all('h3',{'class':'DDPage__header-dek'})[0].contents[0].replace(u'\\xa0', u'')\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    nearby = []\n",
    "    try:\n",
    "        for nearbies in soup.find_all('div',{'class':'DDPageSiderailRecirc__item-text'}):\n",
    "            nearby.append(nearbies.find_all('div',{'class':'DDPageSiderailRecirc__item-title'})[0].contents[0])\n",
    "        scraped['placeNearby'] = nearby\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        address = (str(soup.find_all('aside',{'class':'DDPageSiderail__details'})[0]\n",
    "                           .find_all('address',{'class':'DDPageSiderail__address'})[0]\n",
    "                           .find_all('div')[0])\n",
    "                           .split('\\n', 1)[0])\n",
    "        scraped['placeAddress'] = re.sub('<[^>]*>', \" \", address)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    coordinates = soup.find_all('div',{'class':'DDPageSiderail__coordinates js-copy-coordinates'})[0].contents[2]\n",
    "    scraped['placeAlt'] = float(coordinates.split()[0][:-1])\n",
    "    scraped['placeLong'] = float(coordinates.split()[1])\n",
    "\n",
    "\n",
    "    editorsoup = soup.find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "    scraped['placeEditors'] = [stuff.find_all('span')[0].contents[0] \n",
    "                               for stuff in editorsoup \n",
    "                               if len(stuff.find_all('span')) > 0]\n",
    "    if not scraped['placeEditors']:\n",
    "        zzz = soup.find_all('div',{'class':'ugc-editors'})\n",
    "        flag = 0\n",
    "        for soupper in zzz:\n",
    "            if soupper.find_all('h6')[0].contents[0] == 'Added by':\n",
    "                flag = 1\n",
    "                break\n",
    "        try:\n",
    "            editorsoup = soup.find_all('div',{'class':'ugc-editors'})[flag].find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "            scraped['placeEditors'] = [editors.contents[0]\n",
    "                                       for editors in editorsoup]\n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "    try:\n",
    "        scraped['placePubDate'] = string_to_datetime(soup.find_all('div',{'class':'DDPContributor__name'})[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    kircher = soup.find_all('div',{'class':'athanasius'})\n",
    "    for piece in kircher:\n",
    "        for piecer in piece.find_all('div',{'class':'CardRecircSection__title'}):\n",
    "            if piecer.contents[0] == 'Related Places':\n",
    "                scraped['placeRelatedPlaces'] = [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "            elif 'Appears in' in piecer.contents[0]:\n",
    "                scraped['placeRelatedLists'] =  [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "    \n",
    "    scraped['placeURL'] = 'https://www.atlasobscura.com/places/' + filename[:-4]\n",
    "    \n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb53fea-41ad-4584-84ea-8580afffcb0b",
   "metadata": {},
   "source": [
    "Now we have to define the script that goes through each folder and for each folder goes through each downloaded HTML, scrapes information and store them in a new .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fa2407d-1215-48a3-83a5-313be2db3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,401):\n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "    \n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "        \n",
    "        new_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page) + '\\\\' + filename\n",
    "        soupper = open(new_path, 'r',encoding=\"utf-8\")\n",
    "        \n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv')\n",
    "        newer_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv\\\\'+filename[:-4]+'.tsv'\n",
    "        try:\n",
    "            infos = darkAtlasScraper(soupper)\n",
    "        except IndexError:\n",
    "            print(newer_path)\n",
    "        with open(newer_path,'w+',encoding=\"utf-8\") as new_file:\n",
    "            for info in infos.values():\n",
    "                if type(info) == list:\n",
    "                    for index in range(len(info)):\n",
    "                        if index < len(info) - 1:\n",
    "                            new_file.write(str(info[index])+ ', ')\n",
    "                        elif index == len(info) - 1:\n",
    "                            new_file.write(str(info[index]))\n",
    "                    new_file.write('\\t')\n",
    "                else:\n",
    "                    new_file.write(str(info))\n",
    "                    new_file.write('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a4d4b-29bd-40c4-bb87-ece21839d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Leonardo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= r\"C:\\Users\\Marina\\Downloads\\3HW-ADM-Fabri.Dinino.Aur-main\\3HW-ADM-Fabri.Dinino.Aur-main\\tsv\" \n",
    "final_dataset = pd.DataFrame()\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "for file in filenames:\n",
    "    if file.endswith('tsv'):\n",
    "        file_path = os.path.join(path,file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep =\"\\t\", header=None)\n",
    "            final_dataset = pd.concat([final_dataset,df])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "final_dataset.to_csv('final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdac034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Marina\\Downloads\\3HW-ADM-Fabri.Dinino.Aur-main\\3HW-ADM-Fabri.Dinino.Aur-main\\individual_codes\\final_dataset.csv\"\n",
    "\n",
    "mostPopularPlaces = pd.read_csv(path)\n",
    "mostPopularPlaces.columns = ['placeName', 'placeTags', 'numPeopleVisited', 'numPeopleWant', 'placeDesc', 'placeNearby', \n",
    "        'placeAddress', 'placeAlt', 'placeLong', 'placeEditors', 'placePubDate', 'placeRelatedLists', 'placeRelatedPlaces', 'placeURL', 'placeShortDesc', 'NaN1', 'NaN2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50ebb1-6b01-4e0c-ac62-d6d364b9ba9c",
   "metadata": {},
   "source": [
    "We now have the whole dataset and we are ready to start building the search engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1673fa-cf2d-494b-b215-e3f4468bf727",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1023031",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea00f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de92b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 23.9 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp310-cp310-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 267.7/267.7 kB ? eta 0:00:00\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB ? eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\leonardo\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.7 regex-2022.10.31 tqdm-4.64.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\Leonardo\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\Leonardo\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb97ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntlk_analysis(info):\n",
    "    final_words = []\n",
    "    tokens = word_tokenize(info.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token.encode().isalpha():\n",
    "            stemming_token = ps.stem(token)\n",
    "            final_words.append(stemming_token)\n",
    "            \n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87f2d1",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd48469",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mostPopularPlaces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mostPopularPlaces[\u001b[39m'\u001b[39m\u001b[39mplaceDesc\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mostPopularPlaces' is not defined"
     ]
    }
   ],
   "source": [
    "mostPopularPlaces['placeDesc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514dbc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords = []\n",
    "\n",
    "for index, row in mostPopularPlaces['placeDesc'].iteritems():\n",
    "    description = mostPopularPlaces['placeDesc'][index]\n",
    "    if type(description) == str:\n",
    "        listOfWords.append(ntlk_analysis(description))\n",
    "    else:\n",
    "        listOfWords.append([])\n",
    "    \n",
    "mostPopularPlaces['listOfWords'] = listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9821576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [need, drop, research, facil, exist, address, ...\n",
       "1    [walk, street, forgiven, think, parisian, buil...\n",
       "2    [outsid, bluff, utah, massiv, alcov, loom, san...\n",
       "3    [receiv, fanfar, today, small, town, aurora, t...\n",
       "4    [nestl, ground, floor, unesco, world, heritag,...\n",
       "Name: listOfWords, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostPopularPlaces['listOfWords'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ce52b",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b65b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabolary = {}\n",
    "\n",
    "for index, list_ in mostPopularPlaces['listOfWords'].iteritems():\n",
    "    for word in list_:\n",
    "        if word in vocabolary:\n",
    "            vocabolary[word].append(index)\n",
    "        else:\n",
    "            vocabolary[word] = [index]\n",
    "            \n",
    "with open('vocabolary.txt', 'w') as file:\n",
    "    file.write(str(vocabolary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ed274",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_match(query,df):\n",
    "    result = []\n",
    "    query = ntlk_analysis(query)\n",
    "    match = {key: [] for key in query}\n",
    "\n",
    "    for key,list_of_values in vocabolary.items():\n",
    "        if key in match:\n",
    "            for value in list_of_values:\n",
    "                if value not in match[key]:\n",
    "                    match[key].append(value)\n",
    "                    \n",
    "    final_values = list(match.values())\n",
    "    intersection = set.intersection(*map(set,final_values))\n",
    "    \n",
    "    for index in intersection:\n",
    "        series = df[['placeName', 'placeDesc','placeURL']].loc[index]\n",
    "        result.append(series)\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2eccf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>Harvard Museum of Natural History</td>\n",
       "      <td>Collecting three different institutions into o...</td>\n",
       "      <td>https://www.atlasobscura.com/places/harvard-mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6659</th>\n",
       "      <td>Truth or Consequences</td>\n",
       "      <td>Located along the Rio Grande in middle of the ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/truth-or-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>Catoctin Furnace</td>\n",
       "      <td>In the early 1770s, Thomas Johnson discovered ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/catoctin-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5124</th>\n",
       "      <td>RV/MH Hall of Fame</td>\n",
       "      <td>Almost since the invention of the automobile, ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/rvmh-hall-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>KattenKabinet</td>\n",
       "      <td>The death of a pet can inspire a number of rea...</td>\n",
       "      <td>https://www.atlasobscura.com/places/kattenkabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>Peden's Cave</td>\n",
       "      <td>Found at the top of a red stone cliff overlook...</td>\n",
       "      <td>https://www.atlasobscura.com/places/pedens-cave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>Harriet Beecher Stowe, Slavery to Freedom Museum</td>\n",
       "      <td>This early brick Georgian townhouse sits incon...</td>\n",
       "      <td>https://www.atlasobscura.com/places/harriet-be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>World's Largest Cowboy Boots</td>\n",
       "      <td>In 1979, on a vacant lot three blocks from the...</td>\n",
       "      <td>https://www.atlasobscura.com/places/worlds-lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>Museum of the Weird</td>\n",
       "      <td>The dime or dime store museum is by all accoun...</td>\n",
       "      <td>https://www.atlasobscura.com/places/museum-weird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>Museums at Old City Cemetery</td>\n",
       "      <td>Planning to travel back to 1899 to attend a fu...</td>\n",
       "      <td>https://www.atlasobscura.com/places/museums-at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             placeName  \\\n",
       "2560                 Harvard Museum of Natural History   \n",
       "6659                             Truth or Consequences   \n",
       "1028                                  Catoctin Furnace   \n",
       "5124                                RV/MH Hall of Fame   \n",
       "3078                                     KattenKabinet   \n",
       "...                                                ...   \n",
       "4597                                      Peden's Cave   \n",
       "2550  Harriet Beecher Stowe, Slavery to Freedom Museum   \n",
       "7160                     World's Largest Cowboy Boots    \n",
       "4089                               Museum of the Weird   \n",
       "4090                      Museums at Old City Cemetery   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "2560  Collecting three different institutions into o...   \n",
       "6659  Located along the Rio Grande in middle of the ...   \n",
       "1028  In the early 1770s, Thomas Johnson discovered ...   \n",
       "5124  Almost since the invention of the automobile, ...   \n",
       "3078  The death of a pet can inspire a number of rea...   \n",
       "...                                                 ...   \n",
       "4597  Found at the top of a red stone cliff overlook...   \n",
       "2550  This early brick Georgian townhouse sits incon...   \n",
       "7160  In 1979, on a vacant lot three blocks from the...   \n",
       "4089  The dime or dime store museum is by all accoun...   \n",
       "4090  Planning to travel back to 1899 to attend a fu...   \n",
       "\n",
       "                                               placeURL  \n",
       "2560  https://www.atlasobscura.com/places/harvard-mu...  \n",
       "6659  https://www.atlasobscura.com/places/truth-or-c...  \n",
       "1028  https://www.atlasobscura.com/places/catoctin-f...  \n",
       "5124  https://www.atlasobscura.com/places/rvmh-hall-...  \n",
       "3078  https://www.atlasobscura.com/places/kattenkabinet  \n",
       "...                                                 ...  \n",
       "4597    https://www.atlasobscura.com/places/pedens-cave  \n",
       "2550  https://www.atlasobscura.com/places/harriet-be...  \n",
       "7160  https://www.atlasobscura.com/places/worlds-lar...  \n",
       "4089   https://www.atlasobscura.com/places/museum-weird  \n",
       "4090  https://www.atlasobscura.com/places/museums-at...  \n",
       "\n",
       "[239 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = search_match('american museum', mostPopularPlaces)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55965370",
   "metadata": {},
   "outputs": [],
   "source": [
    "Helloooooooooooooooooo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cdd751ff1a78921eff5cc54f687db2b577836ff9e523662c7eedaeda872ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
