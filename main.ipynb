{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dff634f-8241-4a38-ad49-a3daa42aa70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "import numpy as np\n",
    "\n",
    "import os                                                      #Needed to move between OS folders\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup                                  #Scraper \n",
    "import requests                                                #URL drainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime                                  #To be leveraged to define datetime objects  \n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "import nltk                                                    #Text processing library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')                                         #Download command on NLTK library to get specific \n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    #Useful already implemented tfidf vectorizer from scikit learn library\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep                                  #MapReduce methods to perform the map-shuffle-reduce pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe6532-7b2b-43fc-bbf1-b282d87c4cb4",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c53cba-0b9d-4db4-a286-aaeb7c0bb170",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa368-8991-4b48-8ef8-21f99eff73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "\n",
    "for i in range(1,401):\n",
    "    main_url = 'https://www.atlasobscura.com/places?page=' + str(i) +'&sort=likes_count'\n",
    "    cont = requests.get(main_url)\n",
    "    soup = BeautifulSoup(cont.text)\n",
    "    for place in soup.find_all('a', {'class':'content-card content-card-place'}):\n",
    "        places.append('https://www.atlasobscura.com'+place.get('href'))\n",
    "\n",
    "f = open('places.txt','w+')\n",
    "\n",
    "for place in places:\n",
    "    f.write(place+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d4b30-cacf-4157-bba6-f135886f9b71",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb929137-405c-4738-a256-d8e887c6d351",
   "metadata": {},
   "source": [
    "## 1.2 Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034aa15-b321-47e6-9200-7c5f87340f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('places.txt','r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "dic = {}\n",
    "for i in range(0,7200,18):\n",
    "    dic[1+i//18] = lines[i:i+18]\n",
    "\n",
    "for page in range(0,401):\n",
    "    try:\n",
    "        os.mkdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "\n",
    "    for place in dic[page]:\n",
    "        place_name = place[36:len(place)-1]\n",
    "        vanilla = requests.get(place[:-1],allow_redirects=False,headers = {'User-agent': 'your bot 0.1'})\n",
    "        \n",
    "        with open(place_name+\".txt\",'w+',encoding=\"utf-8\") as new_file:\n",
    "            new_file.write(vanilla.text)\n",
    "\n",
    "os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430dcbd-e617-4b4b-9c90-0796e0cbad59",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0459e40-1e7d-4a14-987b-855837102c60",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a141e-07c6-4ef8-b2d8-7508fb9f5a81",
   "metadata": {},
   "source": [
    "We need to define a script to extract useful information from each HTML we collected. In particular we want these information:\n",
    "1. Place Name (to save as $placeName$): String.\n",
    "2. Place Tags (to save as $placeTags$): List of Strings.\n",
    "3. Number of people who have been there (to save as $numPeopleVisited$): Integer.\n",
    "4. Number of people who want to visit the place(to save as $numPeopleWant$): Integer.\n",
    "5. Description (to save as $placeDesc$): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as $placeShortDesc$): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as $placeNearby$): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as $placeAddress$): String.\n",
    "9. Altitude and Longitude of the place's location(to save as $placeAlt$ and $placeLong$): Integers\n",
    "10. The username of the post editors (to save as $placeEditors$): List of Strings.\n",
    "11. Post publishing date (to save as $placePubDate$): datetime.\n",
    "12. The names of the lists that the place was included in (to save as $placeRelatedLists$): List of Strings.\n",
    "13. The names of the related places (to save as $placeRelatedPlaces$): List of Strings.\n",
    "14. The URL of the page of the place (to save as $placeURL$):String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2494-ef9a-4781-937f-225005b6928e",
   "metadata": {},
   "source": [
    "We leverage *BeautifulSoup library* to scrape the information, but we need just an additional method to convert into a datetime object the post publishing date since it was a string in the format 'Month Day, Year'. For example we found 'May 8, 2010' for the very first link, and instead we wanted '2010/05/08'. This method does so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554ae59c-ecbf-4ad3-bc06-5f5f97e8550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(string):\n",
    "    return str(datetime.strptime(string, '%B %d, %Y').date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba77953-7815-46b1-a045-eb77941ce15d",
   "metadata": {},
   "source": [
    "We define a function that builds a dictionary of information for every place we go through: then from this dictionary we'll buil a .tsv for every HTML document we gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17c331df-bd9f-4ac9-ba0a-ba7f16196880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darkAtlasScraper(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text)\n",
    "    \n",
    "    scraped = {'placeName': 'NaN',\n",
    "               'placeTags': 'NaN',\n",
    "               'numPeopleVisited': 'NaN',\n",
    "               'numPeopleWant': 'NaN',\n",
    "               'placeDesc': 'NaN',\n",
    "               'placeShortDesc':'Nan',\n",
    "               'placeNearby': 'NaN',\n",
    "               'placeAddress': 'NaN',\n",
    "               'placeAlt': 'NaN',\n",
    "               'placeLong': 'NaN',\n",
    "               'placeEditors': 'NaN',\n",
    "               'placePubDate': 'NaN',\n",
    "               'placeRelatedPlaces': 'NaN',\n",
    "               'placeRelatedLists': 'NaN',\n",
    "               'placeURL': 'NaN'}          \n",
    "    \n",
    "    try:\n",
    "        scraped['placeName'] = soup.find_all('h1',{'class':'DDPage__header-title'})[0].contents[0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "           \n",
    "    try:\n",
    "        scraped['placeTags'] = list(map(lambda s:s.strip(),\n",
    "                                        [tag.contents[0] for tag in soup.find_all('a',{'class':'itemTags__link js-item-tags-link'})]))\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    counters = soup.find_all('div',{'class':'title-md item-action-count'})\n",
    "    try:\n",
    "        scraped['numPeopleVisited'] = int(counters[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        scraped['numPeopleWant'] = int(counters[1].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    place_desc = ''\n",
    "    for paragraph in soup.find_all('div',{'class':'DDP__body-copy'})[0].find_all('p'):\n",
    "        for element in paragraph.contents:\n",
    "            if re.search('<[^>]*>', str(element)):\n",
    "                element = re.sub('<[^>]*>', \"\", str(element))\n",
    "                place_desc += element\n",
    "            else:\n",
    "                place_desc += str(element)\n",
    "    scraped['placeDesc'] = place_desc\n",
    "    \n",
    "    try:\n",
    "        scraped['placeShortDesc'] = soup.find_all('h3',{'class':'DDPage__header-dek'})[0].contents[0].replace(u'\\xa0', u'')\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    nearby = []\n",
    "    try:\n",
    "        for nearbies in soup.find_all('div',{'class':'DDPageSiderailRecirc__item-text'}):\n",
    "            nearby.append(nearbies.find_all('div',{'class':'DDPageSiderailRecirc__item-title'})[0].contents[0])\n",
    "        scraped['placeNearby'] = nearby\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        address = (str(soup.find_all('aside',{'class':'DDPageSiderail__details'})[0]\n",
    "                           .find_all('address',{'class':'DDPageSiderail__address'})[0]\n",
    "                           .find_all('div')[0])\n",
    "                           .split('\\n', 1)[0])\n",
    "        scraped['placeAddress'] = re.sub('<[^>]*>', \" \", address)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    coordinates = soup.find_all('div',{'class':'DDPageSiderail__coordinates js-copy-coordinates'})[0].contents[2]\n",
    "    scraped['placeAlt'] = float(coordinates.split()[0][:-1])\n",
    "    scraped['placeLong'] = float(coordinates.split()[1])\n",
    "\n",
    "\n",
    "    editorsoup = soup.find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "    scraped['placeEditors'] = [stuff.find_all('span')[0].contents[0] \n",
    "                               for stuff in editorsoup \n",
    "                               if len(stuff.find_all('span')) > 0]\n",
    "    if not scraped['placeEditors']:\n",
    "        zzz = soup.find_all('div',{'class':'ugc-editors'})\n",
    "        flag = 0\n",
    "        for soupper in zzz:\n",
    "            if soupper.find_all('h6')[0].contents[0] == 'Added by':\n",
    "                flag = 1\n",
    "                break\n",
    "        try:\n",
    "            editorsoup = soup.find_all('div',{'class':'ugc-editors'})[flag].find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "            scraped['placeEditors'] = [editors.contents[0]\n",
    "                                       for editors in editorsoup]\n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "    try:\n",
    "        scraped['placePubDate'] = string_to_datetime(soup.find_all('div',{'class':'DDPContributor__name'})[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    kircher = soup.find_all('div',{'class':'athanasius'})\n",
    "    for piece in kircher:\n",
    "        for piecer in piece.find_all('div',{'class':'CardRecircSection__title'}):\n",
    "            if piecer.contents[0] == 'Related Places':\n",
    "                scraped['placeRelatedPlaces'] = [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "            elif 'Appears in' in piecer.contents[0]:\n",
    "                scraped['placeRelatedLists'] =  [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "    \n",
    "    scraped['placeURL'] = 'https://www.atlasobscura.com/places/' + filename[:-4]\n",
    "    \n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb53fea-41ad-4584-84ea-8580afffcb0b",
   "metadata": {},
   "source": [
    "Now we have to define the script that goes through each folder and for each folder goes through each downloaded HTML, scrapes information and store them in a new .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fa2407d-1215-48a3-83a5-313be2db3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,401):\n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "    \n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "        \n",
    "        new_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page) + '\\\\' + filename\n",
    "        soupper = open(new_path, 'r',encoding=\"utf-8\")\n",
    "        \n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv')\n",
    "        newer_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv\\\\'+filename[:-4]+'.tsv'\n",
    "        try:\n",
    "            infos = darkAtlasScraper(soupper)\n",
    "        except IndexError:\n",
    "            print(newer_path)\n",
    "        with open(newer_path,'w+',encoding=\"utf-8\") as new_file:\n",
    "            for info in infos.values():\n",
    "                if type(info) == list:\n",
    "                    for index in range(len(info)):\n",
    "                        if index < len(info) - 1:\n",
    "                            new_file.write(str(info[index])+ ', ')\n",
    "                        elif index == len(info) - 1:\n",
    "                            new_file.write(str(info[index]))\n",
    "                    new_file.write('\\t')\n",
    "                else:\n",
    "                    new_file.write(str(info))\n",
    "                    new_file.write('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a4d4b-29bd-40c4-bb87-ece21839d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Leonardo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded8d3",
   "metadata": {},
   "source": [
    "We then decided to build a .csv to collect all the data and to make them always available in a practical format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59a1fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= r\"C:\\Users\\Leonardo\\ADM_HW3\\tsv\"\n",
    "final_dataset = []\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "for file in filenames:\n",
    "    if file.endswith('tsv'):\n",
    "        file_path = os.path.join(path,file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep =\"\\t\", header=None, quoting=3)\n",
    "            final_dataset.append(df)\n",
    "        except:\n",
    "            print(file)\n",
    "            pass\n",
    "    \n",
    "final_dataset = pd.concat(final_dataset)\n",
    "final_dataset.to_csv('final_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdac034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Leonardo\\Documents\\GitHub\\3HW-ADM-Fabri.Dinino.Aur\\final_dataset.csv\"\n",
    "\n",
    "mostPopularPlaces = pd.read_csv(path)\n",
    "mostPopularPlaces = mostPopularPlaces.iloc[:, :-2]\n",
    "mostPopularPlaces.columns = ['placeName', 'placeTags', 'numPeopleVisited', 'numPeopleWant', 'placeDesc', 'placeShortDesc', 'placeNearby', \n",
    "        'placeAddress', 'placeAlt', 'placeLong', 'placeEditors', 'placePubDate', 'placeRelatedPlaces', 'placeRelatedLists', 'placeURL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50ebb1-6b01-4e0c-ac62-d6d364b9ba9c",
   "metadata": {},
   "source": [
    "We now have the whole dataset and we are ready to start building the search engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1673fa-cf2d-494b-b215-e3f4468bf727",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1023031",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea00f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')                           #Download command on NLTK library to get specific \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb97ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntlk_analysis(info):\n",
    "    final_words = []\n",
    "    tokens = word_tokenize(info.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token.encode().isalpha():\n",
    "            stemming_token = ps.stem(token)\n",
    "            final_words.append(stemming_token)\n",
    "            \n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87f2d1",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd48469",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords = []\n",
    "for index, row in mostPopularPlaces['placeDesc'].iteritems():\n",
    "    description = mostPopularPlaces['placeDesc'][index]\n",
    "    listOfWords.append(ntlk_analysis(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0027273",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopularPlaces['listOfWords'] = listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9821576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [need, drop, research, facil, exist, address, ...\n",
       "1     [walk, street, forgiven, think, parisian, buil...\n",
       "2     [outsid, bluff, utah, massiv, alcov, loom, san...\n",
       "3     [receiv, fanfar, today, small, town, aurora, t...\n",
       "4     [nestl, ground, floor, unesco, world, heritag,...\n",
       "5     [amidst, glut, shop, restaur, make, san, diego...\n",
       "6     [loui, flight, cage, built, world, fair, year,...\n",
       "7     [commerci, hub, excit, new, way, travel, art, ...\n",
       "8     [home, futur, past, featur, aqua, tile, bathro...\n",
       "9     [pope, john, paul, ii, gift, use, santi, vince...\n",
       "10    [beeton, christma, annual, huge, popular, vict...\n",
       "11    [updat, hour, church, elvi, longer, physic, lo...\n",
       "12    [could, quit, possibl, frivol, yet, excit, adv...\n",
       "13    [els, new, york, citi, would, find, stunningli...\n",
       "14    [airplan, aficionado, rejoic, heaven, found, g...\n",
       "Name: listOfWords, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostPopularPlaces['listOfWords'].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ce52b",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b48d80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "term_id = 1\n",
    "for word in sorted(list(word_occ.keys())):\n",
    "    vocabulary[word] = term_id\n",
    "    term_id += 1\n",
    "\n",
    "with open('vocabolary.txt', 'w') as file:\n",
    "    for word, term_id in vocabulary.items():\n",
    "        file.write('%s:%s\\n' % (word, term_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21b65b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "for index, list_ in mostPopularPlaces['listOfWords'].iteritems():\n",
    "    for word in list_:\n",
    "        if word in inverted_index:\n",
    "            if index not in inverted_index[word]:\n",
    "                inverted_index[word].append(index)\n",
    "        else:\n",
    "            inverted_index[word] = [index]\n",
    "\n",
    "inverted_index = dict(sorted(inverted_index.items()))\n",
    "with open('inverted_index1.txt', 'w') as file:\n",
    "    for word, indexing in inverted_index.items():\n",
    "        file.write('%s:%s\\n' % (word, indexing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ed274",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df9c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_match(query,df):\n",
    "    result = []\n",
    "    query = ntlk_analysis(query)\n",
    "    match = {key: [] for key in query}\n",
    "\n",
    "    for key,list_of_values in vocabolary.items():\n",
    "        if key in match:\n",
    "            for value in list_of_values:\n",
    "                if value not in match[key]:\n",
    "                    match[key].append(value)\n",
    "                    \n",
    "    final_values = list(match.values())\n",
    "    intersection = set.intersection(*map(set,final_values))\n",
    "    \n",
    "    for index in intersection:\n",
    "        series = df[['placeName', 'placeDesc','placeURL']].loc[index]\n",
    "        result.append(series)\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2eccf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>Colossal Squid</td>\n",
       "      <td>Te Papa is New Zealand’s national museum, and ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/colossal-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Rattlesnake Bridge</td>\n",
       "      <td>Tucson, Arizona, is a city with a devotion to ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/diamondbac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Ape Canyon</td>\n",
       "      <td>Ape Canyon is a narrowing gorge sitting just t...</td>\n",
       "      <td>https://www.atlasobscura.com/places/ape-canyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3840</th>\n",
       "      <td>Monster Kabinett</td>\n",
       "      <td>The Monster Kabinett is a wonderland (so to sp...</td>\n",
       "      <td>https://www.atlasobscura.com/places/monster-ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6402</th>\n",
       "      <td>The Wichita Troll</td>\n",
       "      <td>Without a plaque or other indicator, the hidde...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-wichit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3065</th>\n",
       "      <td>Kawaii Monster Cafe</td>\n",
       "      <td>If anyone is responsible for the extravagant, ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/kawaii-mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>Two Bit Circus Micro-Amusement Park</td>\n",
       "      <td>Step right up! Multidisciplinary tech collecti...</td>\n",
       "      <td>https://www.atlasobscura.com/places/two-bit-ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6651</th>\n",
       "      <td>Two Guns</td>\n",
       "      <td>It has all the workings of a modern-day Hamlet...</td>\n",
       "      <td>https://www.atlasobscura.com/places/two-guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>Le Boudoir</td>\n",
       "      <td>It started in 1755, when Marie Antoinette, who...</td>\n",
       "      <td>https://www.atlasobscura.com/places/le-boudoir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>Brooklyn Superhero Supply Store</td>\n",
       "      <td>The Brooklyn Superhero Supply Store is exactly...</td>\n",
       "      <td>https://www.atlasobscura.com/places/brooklyn-s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                placeName  \\\n",
       "1272                       Colossal Squid   \n",
       "1537                   Rattlesnake Bridge   \n",
       "256                            Ape Canyon   \n",
       "3840                     Monster Kabinett   \n",
       "6402                    The Wichita Troll   \n",
       "...                                   ...   \n",
       "3065                  Kawaii Monster Cafe   \n",
       "6650  Two Bit Circus Micro-Amusement Park   \n",
       "6651                             Two Guns   \n",
       "3324                           Le Boudoir   \n",
       "766       Brooklyn Superhero Supply Store   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "1272  Te Papa is New Zealand’s national museum, and ...   \n",
       "1537  Tucson, Arizona, is a city with a devotion to ...   \n",
       "256   Ape Canyon is a narrowing gorge sitting just t...   \n",
       "3840  The Monster Kabinett is a wonderland (so to sp...   \n",
       "6402  Without a plaque or other indicator, the hidde...   \n",
       "...                                                 ...   \n",
       "3065  If anyone is responsible for the extravagant, ...   \n",
       "6650  Step right up! Multidisciplinary tech collecti...   \n",
       "6651  It has all the workings of a modern-day Hamlet...   \n",
       "3324  It started in 1755, when Marie Antoinette, who...   \n",
       "766   The Brooklyn Superhero Supply Store is exactly...   \n",
       "\n",
       "                                               placeURL  \n",
       "1272  https://www.atlasobscura.com/places/colossal-s...  \n",
       "1537  https://www.atlasobscura.com/places/diamondbac...  \n",
       "256      https://www.atlasobscura.com/places/ape-canyon  \n",
       "3840  https://www.atlasobscura.com/places/monster-ka...  \n",
       "6402  https://www.atlasobscura.com/places/the-wichit...  \n",
       "...                                                 ...  \n",
       "3065  https://www.atlasobscura.com/places/kawaii-mon...  \n",
       "6650  https://www.atlasobscura.com/places/two-bit-ci...  \n",
       "6651       https://www.atlasobscura.com/places/two-guns  \n",
       "3324  https://www.atlasobscura.com/places/le-boudoir...  \n",
       "766   https://www.atlasobscura.com/places/brooklyn-s...  \n",
       "\n",
       "[97 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_search_engine = search_match('monster', mostPopularPlaces)\n",
    "first_search_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8113784",
   "metadata": {},
   "source": [
    "# 2.2. Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbf2c3",
   "metadata": {},
   "source": [
    "We can leverage tfidf method from sklearn, or we can implement the tfidf from scratch. We will implement both solution as exercise, but we'll use *TfdifVectorizer* to focus on tuning the model on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20674b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### word_occ = Counter(reduce(lambda x,y:x+y, mostPopularPlaces.listOfWords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cdd751ff1a78921eff5cc54f687db2b577836ff9e523662c7eedaeda872ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
