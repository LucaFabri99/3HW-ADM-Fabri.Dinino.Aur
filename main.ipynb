{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff634f-8241-4a38-ad49-a3daa42aa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "import numpy as np\n",
    "\n",
    "import os                                                      #Needed to move between OS folders\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup                                  #Scraper \n",
    "import requests                                                #URL drainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime                                  #To be leveraged to define datetime objects  \n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "import nltk                                                    #Text processing library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')                                         #Download command on NLTK library to get specific \n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    #Useful already implemented tfidf vectorizer from scikit learn library\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep                                  #MapReduce methods to perform the map-shuffle-reduce pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe6532-7b2b-43fc-bbf1-b282d87c4cb4",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c53cba-0b9d-4db4-a286-aaeb7c0bb170",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa368-8991-4b48-8ef8-21f99eff73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "\n",
    "for i in range(1,401):\n",
    "    main_url = 'https://www.atlasobscura.com/places?page=' + str(i) +'&sort=likes_count'\n",
    "    cont = requests.get(main_url)\n",
    "    soup = BeautifulSoup(cont.text)\n",
    "    for place in soup.find_all('a', {'class':'content-card content-card-place'}):\n",
    "        places.append('https://www.atlasobscura.com'+place.get('href'))\n",
    "\n",
    "f = open('places.txt','w+')\n",
    "\n",
    "for place in places:\n",
    "    f.write(place+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d4b30-cacf-4157-bba6-f135886f9b71",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb929137-405c-4738-a256-d8e887c6d351",
   "metadata": {},
   "source": [
    "## 1.2 Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034aa15-b321-47e6-9200-7c5f87340f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('places.txt','r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "dic = {}\n",
    "for i in range(0,7200,18):\n",
    "    dic[1+i//18] = lines[i:i+18]\n",
    "\n",
    "for page in range(0,401):\n",
    "    try:\n",
    "        os.mkdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "\n",
    "    for place in dic[page]:\n",
    "        place_name = place[36:len(place)-1]\n",
    "        vanilla = requests.get(place[:-1],allow_redirects=False,headers = {'User-agent': 'your bot 0.1'})\n",
    "        \n",
    "        with open(place_name+\".txt\",'w+',encoding=\"utf-8\") as new_file:\n",
    "            new_file.write(vanilla.text)\n",
    "\n",
    "os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430dcbd-e617-4b4b-9c90-0796e0cbad59",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0459e40-1e7d-4a14-987b-855837102c60",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a141e-07c6-4ef8-b2d8-7508fb9f5a81",
   "metadata": {},
   "source": [
    "We need to define a script to extract useful information from each HTML we collected. In particular we want these information:\n",
    "1. Place Name (to save as $placeName$): String.\n",
    "2. Place Tags (to save as $placeTags$): List of Strings.\n",
    "3. Number of people who have been there (to save as $numPeopleVisited$): Integer.\n",
    "4. Number of people who want to visit the place(to save as $numPeopleWant$): Integer.\n",
    "5. Description (to save as $placeDesc$): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as $placeShortDesc$): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as $placeNearby$): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as $placeAddress$): String.\n",
    "9. Altitude and Longitude of the place's location(to save as $placeAlt$ and $placeLong$): Integers\n",
    "10. The username of the post editors (to save as $placeEditors$): List of Strings.\n",
    "11. Post publishing date (to save as $placePubDate$): datetime.\n",
    "12. The names of the lists that the place was included in (to save as $placeRelatedLists$): List of Strings.\n",
    "13. The names of the related places (to save as $placeRelatedPlaces$): List of Strings.\n",
    "14. The URL of the page of the place (to save as $placeURL$):String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2494-ef9a-4781-937f-225005b6928e",
   "metadata": {},
   "source": [
    "We leverage *BeautifulSoup library* to scrape the information, but we need just an additional method to convert into a datetime object the post publishing date since it was a string in the format 'Month Day, Year'. For example we found 'May 8, 2010' for the very first link, and instead we wanted '2010/05/08'. This method does so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ae59c-ecbf-4ad3-bc06-5f5f97e8550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(string):\n",
    "    return str(datetime.strptime(string, '%B %d, %Y').date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba77953-7815-46b1-a045-eb77941ce15d",
   "metadata": {},
   "source": [
    "We define a function that builds a dictionary of information for every place we go through: then from this dictionary we'll buil a .tsv for every HTML document we gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c331df-bd9f-4ac9-ba0a-ba7f16196880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darkAtlasScraper(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text)\n",
    "    \n",
    "    scraped = {'placeName': 'NaN',\n",
    "               'placeTags': 'NaN',\n",
    "               'numPeopleVisited': 'NaN',\n",
    "               'numPeopleWant': 'NaN',\n",
    "               'placeDesc': 'NaN',\n",
    "               'placeShortDesc':'Nan',\n",
    "               'placeNearby': 'NaN',\n",
    "               'placeAddress': 'NaN',\n",
    "               'placeAlt': 'NaN',\n",
    "               'placeLong': 'NaN',\n",
    "               'placeEditors': 'NaN',\n",
    "               'placePubDate': 'NaN',\n",
    "               'placeRelatedPlaces': 'NaN',\n",
    "               'placeRelatedLists': 'NaN',\n",
    "               'placeURL': 'NaN'}          \n",
    "    \n",
    "    try:\n",
    "        scraped['placeName'] = soup.find_all('h1',{'class':'DDPage__header-title'})[0].contents[0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "           \n",
    "    try:\n",
    "        scraped['placeTags'] = list(map(lambda s:s.strip(),\n",
    "                                        [tag.contents[0] for tag in soup.find_all('a',{'class':'itemTags__link js-item-tags-link'})]))\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    counters = soup.find_all('div',{'class':'title-md item-action-count'})\n",
    "    try:\n",
    "        scraped['numPeopleVisited'] = int(counters[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        scraped['numPeopleWant'] = int(counters[1].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    place_desc = ''\n",
    "    for paragraph in soup.find_all('div',{'class':'DDP__body-copy'})[0].find_all('p'):\n",
    "        for element in paragraph.contents:\n",
    "            if re.search('<[^>]*>', str(element)):\n",
    "                element = re.sub('<[^>]*>', \"\", str(element))\n",
    "                place_desc += element\n",
    "            else:\n",
    "                place_desc += str(element)\n",
    "    scraped['placeDesc'] = place_desc\n",
    "    \n",
    "    try:\n",
    "        scraped['placeShortDesc'] = soup.find_all('h3',{'class':'DDPage__header-dek'})[0].contents[0].replace(u'\\xa0', u'')\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    nearby = []\n",
    "    try:\n",
    "        for nearbies in soup.find_all('div',{'class':'DDPageSiderailRecirc__item-text'}):\n",
    "            nearby.append(nearbies.find_all('div',{'class':'DDPageSiderailRecirc__item-title'})[0].contents[0])\n",
    "        scraped['placeNearby'] = nearby\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        address = (str(soup.find_all('aside',{'class':'DDPageSiderail__details'})[0]\n",
    "                           .find_all('address',{'class':'DDPageSiderail__address'})[0]\n",
    "                           .find_all('div')[0])\n",
    "                           .split('\\n', 1)[0])\n",
    "        scraped['placeAddress'] = re.sub('<[^>]*>', \" \", address)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    coordinates = soup.find_all('div',{'class':'DDPageSiderail__coordinates js-copy-coordinates'})[0].contents[2]\n",
    "    scraped['placeAlt'] = float(coordinates.split()[0][:-1])\n",
    "    scraped['placeLong'] = float(coordinates.split()[1])\n",
    "\n",
    "\n",
    "    editorsoup = soup.find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "    scraped['placeEditors'] = [stuff.find_all('span')[0].contents[0] \n",
    "                               for stuff in editorsoup \n",
    "                               if len(stuff.find_all('span')) > 0]\n",
    "    if not scraped['placeEditors']:\n",
    "        zzz = soup.find_all('div',{'class':'ugc-editors'})\n",
    "        flag = 0\n",
    "        for soupper in zzz:\n",
    "            if soupper.find_all('h6')[0].contents[0] == 'Added by':\n",
    "                flag = 1\n",
    "                break\n",
    "        try:\n",
    "            editorsoup = soup.find_all('div',{'class':'ugc-editors'})[flag].find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "            scraped['placeEditors'] = [editors.contents[0]\n",
    "                                       for editors in editorsoup]\n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "    try:\n",
    "        scraped['placePubDate'] = string_to_datetime(soup.find_all('div',{'class':'DDPContributor__name'})[0].contents[0])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    kircher = soup.find_all('div',{'class':'athanasius'})\n",
    "    for piece in kircher:\n",
    "        for piecer in piece.find_all('div',{'class':'CardRecircSection__title'}):\n",
    "            if piecer.contents[0] == 'Related Places':\n",
    "                scraped['placeRelatedPlaces'] = [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "            elif 'Appears in' in piecer.contents[0]:\n",
    "                scraped['placeRelatedLists'] =  [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "    \n",
    "    scraped['placeURL'] = 'https://www.atlasobscura.com/places/' + filename[:-4]\n",
    "    \n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb53fea-41ad-4584-84ea-8580afffcb0b",
   "metadata": {},
   "source": [
    "Now we have to define the script that goes through each folder and for each folder goes through each downloaded HTML, scrapes information and store them in a new .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2407d-1215-48a3-83a5-313be2db3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,401):\n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "    \n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "        \n",
    "        new_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page) + '\\\\' + filename\n",
    "        soupper = open(new_path, 'r',encoding=\"utf-8\")\n",
    "        \n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv')\n",
    "        newer_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv\\\\'+filename[:-4]+'.tsv'\n",
    "        try:\n",
    "            infos = darkAtlasScraper(soupper)\n",
    "        except IndexError:\n",
    "            print(newer_path)\n",
    "        with open(newer_path,'w+',encoding=\"utf-8\") as new_file:\n",
    "            for info in infos.values():\n",
    "                if type(info) == list:\n",
    "                    for index in range(len(info)):\n",
    "                        if index < len(info) - 1:\n",
    "                            new_file.write(str(info[index])+ ', ')\n",
    "                        elif index == len(info) - 1:\n",
    "                            new_file.write(str(info[index]))\n",
    "                    new_file.write('\\t')\n",
    "                else:\n",
    "                    new_file.write(str(info))\n",
    "                    new_file.write('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a4d4b-29bd-40c4-bb87-ece21839d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Leonardo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded8d3",
   "metadata": {},
   "source": [
    "We then decided to build a .csv to collect all the data and to make them always available in a practical format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= r\"C:\\Users\\Leonardo\\ADM_HW3\\tsv\"\n",
    "final_dataset = []\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "for file in filenames:\n",
    "    if file.endswith('tsv'):\n",
    "        file_path = os.path.join(path,file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep =\"\\t\", header=None, quoting=3)\n",
    "            final_dataset.append(df)\n",
    "        except:\n",
    "            print(file)\n",
    "            pass\n",
    "    \n",
    "final_dataset = pd.concat(final_dataset)\n",
    "final_dataset.to_csv('final_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Leonardo\\Documents\\GitHub\\3HW-ADM-Fabri.Dinino.Aur\\final_dataset.csv\"\n",
    "\n",
    "mostPopularPlaces = pd.read_csv(path)\n",
    "mostPopularPlaces = mostPopularPlaces.iloc[:, :-2]\n",
    "mostPopularPlaces.columns = ['placeName', 'placeTags', 'numPeopleVisited', 'numPeopleWant', 'placeDesc', 'placeShortDesc', 'placeNearby', \n",
    "        'placeAddress', 'placeAlt', 'placeLong', 'placeEditors', 'placePubDate', 'placeRelatedPlaces', 'placeRelatedLists', 'placeURL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50ebb1-6b01-4e0c-ac62-d6d364b9ba9c",
   "metadata": {},
   "source": [
    "We now have the whole dataset and we are ready to start building the search engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1673fa-cf2d-494b-b215-e3f4468bf727",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1023031",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')                           #Download command on NLTK library to get specific \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntlk_analysis(info):\n",
    "    final_words = []\n",
    "    tokens = word_tokenize(info.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token.encode().isalpha():\n",
    "            stemming_token = ps.stem(token)\n",
    "            final_words.append(stemming_token)\n",
    "            \n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87f2d1",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd48469",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWords = []\n",
    "for index, row in mostPopularPlaces['placeDesc'].iteritems():\n",
    "    description = mostPopularPlaces['placeDesc'][index]\n",
    "    listOfWords.append(ntlk_analysis(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0027273",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopularPlaces['listOfWords'] = listOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopularPlaces['listOfWords'].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ce52b",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "term_id = 1\n",
    "for word in sorted(list(word_occ.keys())):\n",
    "    vocabulary[word] = term_id\n",
    "    term_id += 1\n",
    "\n",
    "with open('vocabulary.txt', 'w') as file:\n",
    "    for word, term_id in vocabulary.items():\n",
    "        file.write('%s:%s\\n' % (word, term_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c392da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To open the vocabulary from vocabulary.text use this script\n",
    "\n",
    "vocabulary = {}\n",
    "with open('vocabulary.txt', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        word = line.split(':')[0]\n",
    "        term_id = line.split(':')[1]\n",
    "        vocabulary[word] = int(term_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b65b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "for index, list_ in mostPopularPlaces['listOfWords'].iteritems():\n",
    "    for word in list_:\n",
    "        if word in inverted_index:\n",
    "            if index not in inverted_index[word]:\n",
    "                inverted_index[word].append(index)\n",
    "        else:\n",
    "            inverted_index[word] = [index]\n",
    "\n",
    "inverted_index = dict(sorted(inverted_index.items()))\n",
    "with open('inverted_index1.txt', 'w') as file:\n",
    "    for word, indexing in inverted_index.items():\n",
    "        texted_indexing = str(indexing[0]) + ', ' + ', '.join(list(map(str, indexing[1:])))\n",
    "        file.write('%s:%s\\n' % (word, texted_indexing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To open the vocabulary from vocabulary.text use this script\n",
    "\n",
    "inverted_index = {}\n",
    "with open('inverted_index1.txt', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        word = line.split(':')[0]\n",
    "        indexing = line.split(':')[1].strip('\\n')\n",
    "        indexing = indexing.split(',')\n",
    "        if indexing[-1] == ' ':\n",
    "            indexing = indexing[:-1]\n",
    "        inverted_index[word] = list(map(int, [string.strip() for string in indexing]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ed274",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_match(query,df):\n",
    "    result = []\n",
    "    query = ntlk_analysis(query)\n",
    "    match = {key: [] for key in query}\n",
    "\n",
    "    for key,list_of_values in vocabolary.items():\n",
    "        if key in match:\n",
    "            for value in list_of_values:\n",
    "                if value not in match[key]:\n",
    "                    match[key].append(value)\n",
    "                    \n",
    "    final_values = list(match.values())\n",
    "    intersection = set.intersection(*map(set,final_values))\n",
    "    \n",
    "    for index in intersection:\n",
    "        series = df[['placeName', 'placeDesc','placeURL']].loc[index]\n",
    "        result.append(series)\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eccf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_search_engine = search_match('cafe', mostPopularPlaces)\n",
    "first_search_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8113784",
   "metadata": {},
   "source": [
    "# 2.2. Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbf2c3",
   "metadata": {},
   "source": [
    "We can leverage tfidf method from sklearn, or we can implement the tfidf from scratch. We will implement both solution as exercise, but we'll use *TfdifVectorizer* to focus on tuning the model on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20674b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### word_occ = Counter(reduce(lambda x,y:x+y, mostPopularPlaces.listOfWords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cdd751ff1a78921eff5cc54f687db2b577836ff9e523662c7eedaeda872ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
