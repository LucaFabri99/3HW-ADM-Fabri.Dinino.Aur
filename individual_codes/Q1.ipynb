{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff634f-8241-4a38-ad49-a3daa42aa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe6532-7b2b-43fc-bbf1-b282d87c4cb4",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c53cba-0b9d-4db4-a286-aaeb7c0bb170",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa368-8991-4b48-8ef8-21f99eff73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "\n",
    "for i in range(1,401):\n",
    "    main_url = 'https://www.atlasobscura.com/places?page=' + str(i) +'&sort=likes_count'\n",
    "    cont = requests.get(main_url)\n",
    "    soup = BeautifulSoup(cont.text)\n",
    "    for place in soup.find_all('a', {'class':'content-card content-card-place'}):\n",
    "        places.append('https://www.atlasobscura.com'+place.get('href'))\n",
    "\n",
    "f = open('places.txt','w+')\n",
    "\n",
    "for place in places:\n",
    "    f.write(place+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d4b30-cacf-4157-bba6-f135886f9b71",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb929137-405c-4738-a256-d8e887c6d351",
   "metadata": {},
   "source": [
    "## 1.2 Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034aa15-b321-47e6-9200-7c5f87340f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('places.txt','r')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "dic = {}\n",
    "for i in range(0,7200,18):\n",
    "    dic[1+i//18] = lines[i:i+18]\n",
    "\n",
    "for page in range(0,401):\n",
    "    try:\n",
    "        os.mkdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "\n",
    "    for place in dic[page]:\n",
    "        place_name = place[36:len(place)-1]\n",
    "        vanilla = requests.get(place[:-1],allow_redirects=False,headers = {'User-agent': 'your bot 0.1'})\n",
    "        \n",
    "        with open(place_name+\".txt\",'w+',encoding=\"utf-8\") as new_file:\n",
    "            new_file.write(vanilla.text)\n",
    "\n",
    "os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430dcbd-e617-4b4b-9c90-0796e0cbad59",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0459e40-1e7d-4a14-987b-855837102c60",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a141e-07c6-4ef8-b2d8-7508fb9f5a81",
   "metadata": {},
   "source": [
    "We need to define a script to extract useful information from each HTML we collected. In particular we want these information:\n",
    "1. Place Name (to save as $placeName$): String.\n",
    "2. Place Tags (to save as $placeTags$): List of Strings.\n",
    "3. Number of people who have been there (to save as $numPeopleVisited$): Integer.\n",
    "4. Number of people who want to visit the place(to save as $numPeopleWant$): Integer.\n",
    "5. Description (to save as $placeDesc$): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\n",
    "6. Short Description (to save as $placeShortDesc$): String. Everything from the title and location up to the image (blue frame on the example image).\n",
    "7. Nearby Places (to save as $placeNearby$): Extract the names of all nearby places, but only keep unique values: List of Strings.\n",
    "8. Address of the place(to save as $placeAddress$): String.\n",
    "9. Altitude and Longitude of the place's location(to save as $placeAlt$ and $placeLong$): Integers\n",
    "10. The username of the post editors (to save as $placeEditors$): List of Strings.\n",
    "11. Post publishing date (to save as $placePubDate$): datetime.\n",
    "12. The names of the lists that the place was included in (to save as $placeRelatedLists$): List of Strings.\n",
    "13. The names of the related places (to save as $placeRelatedPlaces$): List of Strings.\n",
    "14. The URL of the page of the place (to save as $placeURL$):String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2494-ef9a-4781-937f-225005b6928e",
   "metadata": {},
   "source": [
    "We leverage *BeautifulSoup library* to scrape the information, but we need just an additional method to convert into a datetime object the post publishing date since it was a string in the format 'Month Day, Year'. For example we found 'May 8, 2010' for the very first link, and instead we wanted '2010/05/08'. This method does so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ae59c-ecbf-4ad3-bc06-5f5f97e8550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(string):\n",
    "    return str(datetime.strptime(string, '%B %d, %Y').date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba77953-7815-46b1-a045-eb77941ce15d",
   "metadata": {},
   "source": [
    "We define a function that builds a dictionary of information for every place we go through: then from this dictionary we'll buil a .tsv for every HTML document we gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c331df-bd9f-4ac9-ba0a-ba7f16196880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darkAtlasScraper(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text)\n",
    "    \n",
    "    scraped = {}\n",
    "    scraped['placeName'] = soup.find_all('h1',{'class':'DDPage__header-title'})[0].contents[0]\n",
    "           \n",
    "    scraped['placeTags'] = list(map(lambda s:s.strip(),\n",
    "                                    [tag.contents[0] for tag in soup.find_all('a',{'class':'itemTags__link js-item-tags-link'})]))\n",
    "    \n",
    "\n",
    "    counters = soup.find_all('div',{'class':'title-md item-action-count'})\n",
    "    scraped['numPeopleVisited'] = int(counters[0].contents[0])\n",
    "    scraped['numPeopleWant'] = int(counters[1].contents[0])\n",
    "\n",
    "    place_desc = ''\n",
    "    for paragraph in soup.find_all('div',{'class':'DDP__body-copy'})[0].find_all('p'):\n",
    "        for element in paragraph.contents:\n",
    "            if re.search('<[^>]*>', str(element)):\n",
    "                element = re.sub('<[^>]*>', \"\", str(element))\n",
    "                place_desc += element\n",
    "            else:\n",
    "                place_desc += str(element)\n",
    "    scraped['placeDesc'] = place_desc\n",
    "\n",
    "    scraped['placeShortDesc'] = soup.find_all('h3',{'class':'DDPage__header-dek'})[0].contents[0].replace(u'\\xa0', u'')\n",
    "\n",
    "    nearby = []\n",
    "    for nearbies in soup.find_all('div',{'class':'DDPageSiderailRecirc__item-text'}):\n",
    "        nearby.append(nearbies.find_all('div',{'class':'DDPageSiderailRecirc__item-title'})[0].contents[0])\n",
    "    scraped['placeNearby'] = nearby\n",
    "\n",
    "    address = (str(soup.find_all('aside',{'class':'DDPageSiderail__details'})[0]\n",
    "                   .find_all('address',{'class':'DDPageSiderail__address'})[0]\n",
    "                   .find_all('div')[0])\n",
    "                   .split('\\n', 1)[0])\n",
    "    scraped['placeAddress'] = re.sub('<[^>]*>', \" \", address)\n",
    "\n",
    "    coordinates = soup.find_all('div',{'class':'DDPageSiderail__coordinates js-copy-coordinates'})[0].contents[2]\n",
    "    scraped['placeAlt'] = float(coordinates.split()[0][:-1])\n",
    "    scraped['placeLong'] = float(coordinates.split()[1])\n",
    "\n",
    "\n",
    "    editorsoup = soup.find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "    scraped['placeEditors'] = [stuff.find_all('span')[0].contents[0] \n",
    "                               for stuff in editorsoup \n",
    "                               if len(stuff.find_all('span')) > 0]\n",
    "    if not scraped['placeEditors']:\n",
    "        editorsoup = soup.find_all('div',{'class':'ugc-editors'})[0].find_all('a',{'class':'DDPContributorsList__contributor'})\n",
    "        scraped['placeEditors'] = [editors.contents[0]\n",
    "                                   for editors in editorsoup]\n",
    "    if not scraped['placeEditors']:\n",
    "        scraped['placeEditors'] = ''\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        scraped['placePubDate'] = string_to_datetime(soup.find_all('div',{'class':'DDPContributor__name'})[0].contents[0])\n",
    "    except IndexError:\n",
    "        scraped['placePubDate'] = ''\n",
    "\n",
    "    kircher = soup.find_all('div',{'class':'athanasius'})\n",
    "    for piece in kircher:\n",
    "        for piecer in piece.find_all('div',{'class':'CardRecircSection__title'}):\n",
    "            if piecer.contents[0] == 'Related Places':\n",
    "                scraped['placeRelatedPlaces'] = [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "            elif 'Appears in' in piecer.contents[0]:\n",
    "                scraped['placeRelatedLists'] =  [re.sub('<[^>]*>', \"\", str(chunk.contents[1])) \n",
    "                                                 for chunk in piece.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "    \n",
    "    scraped['placeURL'] = 'https://www.atlasobscura.com/places/' + filename[:-4]\n",
    "    \n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb53fea-41ad-4584-84ea-8580afffcb0b",
   "metadata": {},
   "source": [
    "Now we have to define the script that goes through each folder and for each folder goes through each downloaded HTML, scrapes information and store them in a new .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2407d-1215-48a3-83a5-313be2db3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,401):\n",
    "    path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page)\n",
    "    os.chdir(path)\n",
    "    \n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page))\n",
    "        \n",
    "        new_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\page ' + str(page) + '\\\\' + filename\n",
    "        soupper = open(new_path, 'r',encoding=\"utf-8\")\n",
    "        \n",
    "        os.chdir(r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv')\n",
    "        newer_path = r'C:\\\\Users\\\\Leonardo\\\\ADM_HW3\\\\tsv\\\\'+filename[:-4]+'.tsv'\n",
    "        try:\n",
    "            infos = darkAtlasScraper(soupper)\n",
    "        except IndexError:\n",
    "            print(newer_path)\n",
    "        with open(newer_path,'w+',encoding=\"utf-8\") as new_file:\n",
    "            for info in infos.values():\n",
    "                if type(info) == list:\n",
    "                    for index in range(len(info)):\n",
    "                        if index < len(info) - 1:\n",
    "                            new_file.write(str(info[index])+ ', ')\n",
    "                        elif index == len(info) - 1:\n",
    "                            new_file.write(str(info[index]))\n",
    "                    new_file.write('\\t')\n",
    "                else:\n",
    "                    new_file.write(str(info))\n",
    "                    new_file.write('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a4d4b-29bd-40c4-bb87-ece21839d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Leonardo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50ebb1-6b01-4e0c-ac62-d6d364b9ba9c",
   "metadata": {},
   "source": [
    "We now have the whole dataset and we are ready to start building the search engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1673fa-cf2d-494b-b215-e3f4468bf727",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
