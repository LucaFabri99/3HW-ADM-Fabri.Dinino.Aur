# 3HW-ADM-Fabri.Dinino.Aur

## Group Members
* Marina
* Leonardo 
* Luca 

## HW's goals
We were asked to create our own dataset by scraping the [Atlas Obsura](https://www.atlasobscura.com/) website, an American online magazine and travel firm, catalogues unusual and obscure tourist locations. The goals of the Homework are:

* be introduced to crawling and parsing of html documents;
* learn how to create a search engine that whould suggets locations based on the user's query and how to use metrics to evalute its output (such as cosine similarity, tfIdf score and Jaccard similarity);
* become familiar with the command line;
* make acquaintance of sorting algorithms.

## Files description
In this repository you'll find:

### 1. `stuff`

#### A folder in which they are loacted some useful files. 

### 2. `main.ipynb`

#### A Jupyter notebook that contains all our answers and scripts relative to the HW's questions. 

### 2. `scraper.py`

#### The first part of the homework has been runned locally in Leonardo's machine. It was about gathering the dataset through some scraping scripts, so following the guidelines we drained the links and then downloaded the HTML and parsed them, targeting some specific information. All the methods implied in this first part are in the **scraper.py** file, and in the *stuff* folder we can find **final_dataset.csv**, a dataset we build with the required information in order to deal with more practical operations.

### 3. `engine.py`

#### Description

### 4. `command_line.sh`

##### A bash shell script file that contains the prepared script to answer to the command line question.

## Additional link for work visualization
